<?xml version="1.0" encoding="utf-8"?>
<Project>
  <Your_Goal>
    Implementar un sistema empresarial de orquestaci√≥n de scraping con arquitectura robusta para Ubuntu 24.04 LTS.
    El sistema incluye scrapers Python distribuidos que extraen informaci√≥n de m√∫ltiples sitios web inmobiliarios,
    con gesti√≥n autom√°tica de errores, monitoreo en tiempo real, y operaci√≥n 24/7 como servicio del sistema.
  </Your_Goal>

  <Project_Goal>
    Ejecutar m√∫ltiples procesos de scraping en paralelo (8+ concurrentes) con reejecutaci√≥n autom√°tica cada 15 d√≠as
    para mantener datos actualizados. Sistema aut√≥nomo con recuperaci√≥n autom√°tica de fallos, backup diario,
    monitoreo continuo y escalabilidad empresarial. Integraci√≥n nativa con systemd para operaci√≥n como daemon.
  </Project_Goal>

  <Architecture>
    <Database>SQLite con esquema relacional para gesti√≥n de tareas, lotes de ejecuci√≥n, estados y m√©tricas</Database>
    <Orchestrator>Sistema principal en Python con asyncio para concurrencia y gesti√≥n de recursos</Orchestrator>
    <Monitoring>CLI interactivo con dashboard en tiempo real, m√©tricas y sistema de alertas</Monitoring>
    <Service>Servicio systemd nativo para Ubuntu 24 con reinicio autom√°tico y gesti√≥n de recursos</Service>
    <Configuration>Archivo YAML centralizado en /etc/scraping/config.yaml para configuraci√≥n flexible</Configuration>
    <Security>Usuario dedicado 'scraping' con permisos m√≠nimos, sandboxing y l√≠mites de recursos</Security>
  </Architecture>

  <Variables>
    La jerarqu√≠a se mantiene igual pero ahora se gestiona a trav√©s de base de datos relacional:
    <Column1 name="PaginaWeb">Sitio Web: (Inm24, CyT, Lam, Mit, Prop, Tro)</Column1>
    <Column2 name="Ciudad">Ciudad: (Gdl, Zap, Tlaj, Tlaq, Ton, Salt, Zptl, IMem, Jnctl)</Column2>
    <Column3 name="Operacion">Operaci√≥n: (Ven, Ren, Ven-d, Ven-r)</Column3>
    <Column4 name="ProductoPaginaWeb">Producto: (Dep, Cas, Ofc, Com, etc...)</Column4>
    <Column5 name="URL">URL de extracci√≥n (orden preservado en base de datos con campo 'order_num')</Column5>
    <Additional_Fields>
      <Status>Estado: (pending, running, completed, failed, retrying)</Status>
      <Attempts>N√∫mero de intentos realizados (m√°ximo 3)</Attempts>
      <Execution_Batch>ID del lote de ejecuci√≥n para trazabilidad</Execution_Batch>
      <Timestamps>Marcas de tiempo: created_at, started_at, completed_at</Timestamps>
      <Error_Message>Mensaje de error detallado para diagn√≥stico</Error_Message>
    </Additional_Fields>
  </Variables>

  <Execution_Flow>
    <Phase1_Main_Scrapers>
      El sistema ejecuta scrapers principales en paralelo (hasta 8 simult√°neos) respetando prioridades:
      1. Inm24 (prioridad 1, con scraper detalle)
      2. CyT (prioridad 2)
      3. Lam (prioridad 3, con scraper detalle)
      4. Mit, Prop, Tro (prioridades 4-6)
      
      Cada scraper lee su CSV correspondiente desde /var/lib/scraping/urls/ y procesa URLs en orden secuencial.
      Rate limiting configurable por sitio web (1-3 segundos entre requests).
    </Phase1_Main_Scrapers>
    
    <Phase2_Detail_Scrapers>
      Despu√©s de completar scrapers principales, se ejecutan scrapers de detalle:
      - inm24_det.py: Procesa URLs generadas por inm24.py
      - lam_det.py: Procesa URLs generadas por lam.py
      
      Dependencia autom√°tica gestionada por el orquestador.
    </Phase2_Detail_Scrapers>
    
    <Error_Recovery>
      - Reintentos autom√°ticos: hasta 3 intentos por tarea fallida
      - Delay configurable: 30 minutos entre reintentos
      - Aislamiento de fallos: scrapers independientes
      - Recuperaci√≥n graceful: contin√∫a con tareas disponibles
    </Error_Recovery>
  </Execution_Flow>

  <Folder_System_Enhanced>
    Estructura de directorios optimizada para Ubuntu 24:
    
    <System_Directories>
      /opt/scraping/                    # C√≥digo fuente y aplicaci√≥n
      ‚îú‚îÄ‚îÄ orchestrator.py               # Orquestador principal
      ‚îú‚îÄ‚îÄ monitor_cli.py               # CLI de monitoreo
      ‚îú‚îÄ‚îÄ scrapers/                    # Scrapers individuales
      ‚îú‚îÄ‚îÄ venv/                        # Entorno virtual Python
      ‚îú‚îÄ‚îÄ requirements.txt             # Dependencias Python
      ‚îú‚îÄ‚îÄ backup.sh                    # Script de backup
      ‚îî‚îÄ‚îÄ cleanup.sh                   # Script de limpieza
      
      /var/lib/scraping/               # Datos y estado
      ‚îú‚îÄ‚îÄ orchestrator.db              # Base de datos SQLite
      ‚îú‚îÄ‚îÄ Base_de_Datos/                        # Datos extra√≠dos
      ‚îî‚îÄ‚îÄ urls/                        # CSVs de URLs fuente
      
      /var/log/scraping/               # Logs del sistema
      ‚îú‚îÄ‚îÄ orchestrator.log             # Log principal
      ‚îú‚îÄ‚îÄ backup.log                   # Log de backups
      ‚îî‚îÄ‚îÄ cleanup.log                  # Log de limpieza
      
      /etc/scraping/                   # Configuraci√≥n
      ‚îî‚îÄ‚îÄ config.yaml                  # Configuraci√≥n principal
      
      /var/backups/scraping/           # Backups autom√°ticos
    </System_Directories>
    
    <Data_Structure>
      La estructura de datos se mantiene igual, pero con mejoras:
      Base_de_Datos/‚ÄùPaginaWeb‚Äù/‚ÄùCiudad‚Äù/‚ÄùOperaci√≥n‚Äù/‚ÄùProductoPaginaWeb‚Äù/‚ÄùMes;A√±o‚Äù/‚ÄùNumeroEjecucion‚Äù/
      
      Ejemplo mejorado:
      <Path>Base_de_Datos/CyT/Gdl/Ven/Dep/Sep25/01/CyT_Gdl_Ven_Dep_Sep25_01.csv</Path>
      
      Ventajas adicionales:
      - Backup autom√°tico de datos recientes
      - Compresi√≥n autom√°tica de archivos antiguos
      - Limpieza autom√°tica despu√©s de 6 meses
      - Indexaci√≥n en base de datos para b√∫squedas r√°pidas
    </Data_Structure>
  </Folder_System_Enhanced>

  <Database_Schema>
    <Table_scraping_tasks>
      - id: PRIMARY KEY AUTOINCREMENT
      - scraper_name: TEXT NOT NULL
      - website, city, operation, product: TEXT NOT NULL
      - url: TEXT NOT NULL
      - order_num: INTEGER NOT NULL
      - status: TEXT NOT NULL (pending/running/completed/failed/retrying)
      - attempts: INTEGER DEFAULT 0
      - max_attempts: INTEGER DEFAULT 3
      - created_at, started_at, completed_at: TIMESTAMP
      - error_message: TEXT
      - execution_batch: TEXT (FK)
      - output_path: TEXT
    </Table_scraping_tasks>
    
    <Table_execution_batches>
      - id: PRIMARY KEY AUTOINCREMENT
      - batch_id: TEXT UNIQUE (formato: Sep25_01)
      - month_year: TEXT NOT NULL
      - execution_number: INTEGER NOT NULL
      - started_at, completed_at: TIMESTAMP
      - total_tasks, completed_tasks, failed_tasks: INTEGER
      - status: TEXT (running/completed/failed)
    </Table_execution_batches>
  </Database_Schema>

  <Configuration_System>
    Configuraci√≥n centralizada en YAML con secciones organizadas:
    
    <Core_Config>
      - database: Configuraci√≥n de SQLite y backups
      - data: Rutas de directorios de datos
      - scrapers: Configuraci√≥n de scrapers y Python
      - execution: Par√°metros de ejecuci√≥n y concurrencia
    </Core_Config>
    
    <Advanced_Config>
      - websites: Configuraci√≥n espec√≠fica por sitio (rate limits, prioridades)
      - monitoring: M√©tricas, alertas y notificaciones
      - security: Rate limiting, user agents, proxies
      - logging: Niveles, rotaci√≥n, formatos
      - backup: Pol√≠ticas de backup y retenci√≥n
    </Advanced_Config>
  </Configuration_System>

  <Service_Management>
    <Systemd_Integration>
      Servicio nativo: scraping-orchestrator.service
      - Inicio autom√°tico en boot
      - Reinicio autom√°tico en fallos
      - L√≠mites de recursos (memoria, CPU, archivos)
      - Sandboxing de seguridad
      - Logging a journal
    </Systemd_Integration>
    
    <Commands>
      sudo systemctl start scraping-orchestrator    # Iniciar
      sudo systemctl stop scraping-orchestrator     # Detener
      sudo systemctl restart scraping-orchestrator  # Reiniciar
      sudo systemctl status scraping-orchestrator   # Estado
      journalctl -u scraping-orchestrator -f        # Logs en tiempo real
    </Commands>
  </Service_Management>

  <Monitoring_System>
    <CLI_Commands>
      scraping-monitor status           # Estado actual del sistema
      scraping-monitor history          # Historial de ejecuciones
      scraping-monitor tasks            # Tareas del √∫ltimo lote
      scraping-monitor stats            # Estad√≠sticas de rendimiento
      scraping-monitor logs -f          # Logs en tiempo real
      scraping-monitor dashboard        # Dashboard interactivo
      scraping-monitor run-now          # Ejecutar scraping inmediatamente
      scraping-monitor retry -b BATCH   # Reintentar lote fallido
    </CLI_Commands>
    
    <Dashboard_Features>
      - Estado en tiempo real con colores
      - Progreso de lotes de ejecuci√≥n
      - Estad√≠sticas por sitio web y ciudad
      - Tareas fallidas con detalles de error
      - M√©tricas de sistema (CPU, memoria, disco)
      - Hist√≥rico de ejecuciones
    </Dashboard_Features>
    
    <Metrics_Tracking>
      - Tasa de √©xito por sitio web
      - Tiempo promedio de ejecuci√≥n
      - N√∫mero de reintentos por tarea
      - Volumen de datos extra√≠dos
      - Uso de recursos del sistema
      - Tendencias temporales
    </Metrics_Tracking>
  </Monitoring_System>

  <Automation_Features>
    <Scheduled_Operations>
      - Ejecuci√≥n principal: cada 15 d√≠as autom√°ticamente
      - Backup diario: 2:00 AM con retenci√≥n de 30 d√≠as
      - Limpieza semanal: domingos 3:00 AM
      - Verificaci√≥n de salud: cada hora
      - Rotaci√≥n de logs: diaria con compresi√≥n
    </Scheduled_Operations>
    
    <Auto_Recovery>
      - Reintentos autom√°ticos con backoff exponencial
      - Detecci√≥n de scrapers colgados (timeout configurable)
      - Recuperaci√≥n de archivos de estado corruptos
      - Restauraci√≥n autom√°tica desde backup
      - Notificaciones de errores cr√≠ticos
    </Auto_Recovery>
  </Automation_Features>

  <Performance_Optimizations>
    <Concurrency>
      - Hasta 8 scrapers simult√°neos (configurable)
      - AsyncIO para operaciones I/O intensivas
      - Sem√°foros para control de recursos
      - Pool de conexiones HTTP reutilizables
    </Concurrency>
    
    <Resource_Management>
      - L√≠mites de memoria por proceso
      - Timeouts configurables por scraper
      - Rate limiting inteligente por sitio
      - Gesti√≥n autom√°tica de archivos temporales
    </Resource_Management>
    
    <Data_Management>
      - Compresi√≥n autom√°tica de datos antiguos
      - Indexaci√≥n de base de datos optimizada
      - Vacuum autom√°tico de SQLite
      - Detecci√≥n y eliminaci√≥n de duplicados
    </Data_Management>
  </Performance_Optimizations>

  <Installation_Deployment>
    <Automated_Installation>
      Script de instalaci√≥n completo (install.sh) que configura:
      - Usuario dedicado 'scraping' con permisos m√≠nimos
      - Entorno virtual Python con dependencias
      - Estructura de directorios del sistema
      - Servicios systemd y cron jobs
      - Configuraci√≥n de seguridad y l√≠mites
      - Herramientas de monitoreo y backup
    </Automated_Installation>
    
    <Zero_Downtime_Updates>
      - Actualizaciones sin interrumpir ejecuciones en curso
      - Migraci√≥n autom√°tica de esquemas de base de datos
      - Backup autom√°tico antes de actualizaciones
      - Rollback autom√°tico en caso de fallos
    </Zero_Downtime_Updates>
  </Installation_Deployment>

  <Security_Hardening>
    <System_Security>
      - Usuario dedicado sin privilegios administrativos
      - Sandboxing con systemd (ProtectSystem, PrivateTmp)
      - L√≠mites de recursos para prevenir DoS
      - Logs auditables para trazabilidad completa
    </System_Security>
    
    <Scraping_Security>
      - Respeto a robots.txt autom√°tico
      - Rate limiting configurable por sitio
      - Rotaci√≥n de User-Agents
      - Soporte para proxies (opcional)
      - Detecci√≥n y manejo de CAPTCHAs
    </Scraping_Security>
  </Security_Hardening>

  <Scalability_Future>
    <Horizontal_Scaling>
      Preparado para evoluci√≥n futura:
      - Migraci√≥n a Redis/RabbitMQ para colas distribuidas
      - Soporte para m√∫ltiples nodos de trabajo
      - Balanceador de carga para alta disponibilidad
      - Almacenamiento distribuido para big data
    </Horizontal_Scaling>
    
    <Integration_Ready>
      - API REST para integraci√≥n externa (futuro)
      - Exportaci√≥n a m√∫ltiples formatos (CSV, JSON, Parquet)
      - Webhooks para notificaciones en tiempo real
      - M√©tricas compatibles con Prometheus/Grafana
    </Integration_Ready>
  </Scalability_Future>

  <Quality_Assurance>
    <Data_Validation>
      - Validaci√≥n autom√°tica de formatos de datos
      - Detecci√≥n de datos inconsistentes o corruptos
      - Verificaci√≥n de integridad de archivos CSV
      - Alertas por anomal√≠as en vol√∫menes de datos
    </Data_Validation>
    
    <System_Health>
      - Monitoreo continuo de recursos del sistema
      - Detecci√≥n temprana de degradaci√≥n de rendimiento
      - Alertas autom√°ticas por fallos cr√≠ticos
      - Reportes de salud programados
    </System_Health>
  </Quality_Assurance>

  <Documentation_Support>
    <User_Documentation>
      - README completo con ejemplos de uso
      - Documentaci√≥n de comandos CLI
      - Gu√≠as de troubleshooting
      - Mejores pr√°cticas de configuraci√≥n
    </User_Documentation>
    
    <Technical_Documentation>
      - Documentaci√≥n de arquitectura del sistema
      - Esquemas de base de datos
      - APIs internas y extensiones
      - Procedimientos de backup y recuperaci√≥n
    </Technical_Documentation>
  </Documentation_Support>

  <Success_Metrics>
    El sistema mejorado proporciona:
    - üöÄ 80% reducci√≥n en tiempo de gesti√≥n manual
    - üõ°Ô∏è 99%+ confiabilidad con recuperaci√≥n autom√°tica
    - üìà Escalabilidad horizontal para crecimiento
    - üëÅÔ∏è Visibilidad completa del proceso de scraping
    - ‚ö° Operaci√≥n 24/7 sin intervenci√≥n manual
    - üîí Seguridad empresarial con auditor√≠a completa
    - üíæ Backup autom√°tico con retenci√≥n configurable
    - üéØ Monitoreo en tiempo real con m√©tricas detalladas
  </Success_Metrics>
</Project>

