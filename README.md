# üèóÔ∏è Sistema de Orquestaci√≥n de Scraping Inmobiliario

[![Python Version](https://img.shields.io/badge/python-3.12%2B-blue.svg)](https://python.org)
[![Platform](https://img.shields.io/badge/platform-Ubuntu%2024.04-orange.svg)](https://ubuntu.com)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/status-Production%20Ready-brightgreen.svg)](https://github.com)

## üìã Tabla de Contenidos

- [Descripci√≥n General](#-descripci√≥n-general)
- [Caracter√≠sticas Principales](#-caracter√≠sticas-principales)
- [Arquitectura del Sistema](#Ô∏è-arquitectura-del-sistema)
- [Instalaci√≥n](#-instalaci√≥n)
- [Configuraci√≥n](#Ô∏è-configuraci√≥n)
- [Uso](#-uso)
 - [Almacenamiento de Datos SQL](#-almacenamiento-de-datos-sql)
- [Monitoreo](#-monitoreo)
- [API y CLI](#-api-y-cli)
- [Desarrollo](#-desarrollo)
- [Troubleshooting](#-troubleshooting)
- [Contribuci√≥n](#-contribuci√≥n)
- [Licencia](#-licencia)

## üéØ Descripci√≥n General

El **Sistema de Orquestaci√≥n de Scraping Inmobiliario** es una plataforma empresarial que automatiza la extracci√≥n coordinada de datos inmobiliarios de m√∫ltiples sitios web mexicanos. Dise√±ado para operar 24/7 con alta confiabilidad, monitoreo en tiempo real y recuperaci√≥n autom√°tica de errores.

### üèòÔ∏è Sitios Web Soportados

| Sitio Web | C√≥digo | Detalle | Prioridad | Estado |
|-----------|--------|---------|-----------|---------|
| Inmuebles24 | `Inm24` | ‚úÖ S√≠ | 1 (Alta) | ‚úÖ Activo |
| CasasyTerrenos | `CyT` | ‚ùå No | 2 | ‚úÖ Activo |
| Lamudi | `Lam` | ‚úÖ S√≠ | 3 | ‚úÖ Activo |
| Mitula | `Mit` | ‚ùå No | 4 | ‚úÖ Activo |
| Propiedades.com | `Prop` | ‚ùå No | 5 | ‚úÖ Activo |
| Trovit | `Tro` | ‚ùå No | 6 | ‚úÖ Activo |

### üåç Cobertura Geogr√°fica

**Zona Metropolitana de Guadalajara:**
- Guadalajara (`Gdl`)
- Zapopan (`Zap`) 
- Tlajomulco (`Tlaj`)
- Tlaquepaque (`Tlaq`)
- Tonal√° (`Ton`)
- El Salto (`Salt`)

## ‚ú® Caracter√≠sticas Principales

### üöÄ **Orquestaci√≥n Inteligente**
- **Ejecuci√≥n paralela**: Hasta 8 scrapers simult√°neos
- **Dependencias autom√°ticas**: Scrapers de detalle esperan a principales
- **Priorizaci√≥n**: Orden de ejecuci√≥n basado en importancia
- **Rate limiting**: Control de velocidad por sitio web

### üõ°Ô∏è **Confiabilidad Empresarial**
- **Reintentos autom√°ticos**: Hasta 3 intentos por tarea fallida
- **Recuperaci√≥n graceful**: Contin√∫a operaci√≥n aunque fallen scrapers
- **Timeout inteligente**: Previene procesos colgados
- **Aislamiento de fallos**: Un error no detiene el sistema

### üìä **Monitoreo y Observabilidad**
- **Dashboard en tiempo real**: Estado visual del sistema
- **M√©tricas detalladas**: Tasas de √©xito, tiempos, vol√∫menes
- **Alertas configurables**: Notificaciones por email/Slack
- **Historial completo**: Trazabilidad de todas las ejecuciones

### üíæ **Gesti√≥n de Datos**
- **Organizaci√≥n jer√°rquica**: Estructura est√°ndar de carpetas
- **Versionado autom√°tico**: Control de versiones por ejecuci√≥n
- **Backup programado**: Respaldo autom√°tico diario
- **Limpieza inteligente**: Archivos antiguos se comprimen/eliminan

### üîß **Facilidad de Uso**
- **Configuraci√≥n YAML**: Par√°metros centralizados
- **CLI rico**: Comandos intuitivos para todas las operaciones
- **Menu interactivo**: Interfaz amigable para usuarios no t√©cnicos
- **Documentaci√≥n integrada**: Ayuda contextual en todos los comandos

## üèóÔ∏è Arquitectura del Sistema

### üìê **Diagrama de Arquitectura**

```mermaid
graph TB
    subgraph "Interface Layer"
        CLI[Monitor CLI]
        MENU[Menu Interactivo]
        SCRIPTS[Scripts .bat]
    end
    
    subgraph "Orchestration Layer"
        ORCH[Orchestrator]
        ADAPTER[Scraper Adapter]
        SCHEDULER[Task Scheduler]
    end
    
    subgraph "Data Layer"
        DB[(SQLite DB)]
        FILES[File System]
        LOGS[Log Files]
    end
    
    subgraph "Scraping Layer"
        INM24[inm24.py]
        CYT[cyt.py]
        LAM[lam.py]
        MIT[mit.py]
        PROP[prop.py]
        TRO[tro.py]
        INM24D[inm24_det.py]
        LAMD[lam_det.py]
    end
    
    subgraph "External"
        WEB1[Inmuebles24]
        WEB2[CasasyTerrenos]
        WEB3[Lamudi]
        WEB4[Mitula]
        WEB5[Propiedades]
        WEB6[Trovit]
    end
    
    CLI --> ORCH
    MENU --> ORCH
    SCRIPTS --> ORCH
    
    ORCH --> ADAPTER
    ORCH --> SCHEDULER
    ORCH --> DB
    
    ADAPTER --> INM24
    ADAPTER --> CYT
    ADAPTER --> LAM
    ADAPTER --> MIT
    ADAPTER --> PROP
    ADAPTER --> TRO
    
    INM24 --> INM24D
    LAM --> LAMD
    
    INM24 --> WEB1
    CYT --> WEB2
    LAM --> WEB3
    MIT --> WEB4
    PROP --> WEB5
    TRO --> WEB6
    
    ORCH --> FILES
    ORCH --> LOGS
```

### üß© **Componentes Principales**

#### **1. Orchestrator Core (`orchestrator.py`)**
- **Responsabilidad**: Coordinaci√≥n central de todas las operaciones
- **Funciones**:
  - Gesti√≥n de lotes de ejecuci√≥n
  - Control de concurrencia y recursos
  - Manejo de errores y reintentos
  - Interacci√≥n con base de datos

#### **2. Scraper Adapter (`improved_scraper_adapter.py`)**
- **Responsabilidad**: Integraci√≥n de scrapers existentes
- **Funciones**:
  - Adaptaci√≥n din√°mica de scrapers legacy
  - Inyecci√≥n de configuraci√≥n
  - Normalizaci√≥n de salidas
  - Manejo de dependencias entre scrapers

#### **3. Monitor CLI (`monitor_cli.py`)**
- **Responsabilidad**: Interfaz de monitoreo y control
- **Funciones**:
  - Dashboard en tiempo real
  - Comandos de administraci√≥n
  - Visualizaci√≥n de m√©tricas
  - Generaci√≥n de reportes

#### **4. System Validator (`validate_system.py`)**
- **Responsabilidad**: Validaci√≥n y diagn√≥stico
- **Funciones**:
  - Verificaci√≥n de dependencias
  - Validaci√≥n de configuraci√≥n
  - Diagn√≥stico de problemas
  - Generaci√≥n de reportes de salud

### üóÑÔ∏è **Esquema de Base de Datos**

```sql
-- Tabla principal de tareas de scraping
CREATE TABLE scraping_tasks (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    scraper_name TEXT NOT NULL,           -- Nombre del scraper
    website TEXT NOT NULL,               -- C√≥digo del sitio web
    city TEXT NOT NULL,                  -- C√≥digo de ciudad
    operation TEXT NOT NULL,             -- Tipo de operaci√≥n
    product TEXT NOT NULL,               -- Tipo de producto
    url TEXT NOT NULL,                   -- URL a scrapear
    order_num INTEGER NOT NULL,          -- Orden de ejecuci√≥n
    status TEXT NOT NULL,                -- Estado actual
    attempts INTEGER DEFAULT 0,          -- N√∫mero de intentos
    max_attempts INTEGER DEFAULT 3,      -- M√°ximo de intentos
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    error_message TEXT,                  -- Mensaje de error
    execution_batch TEXT,                -- ID del lote
    output_path TEXT                     -- Ruta del archivo generado
);

-- Tabla de lotes de ejecuci√≥n
CREATE TABLE execution_batches (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    batch_id TEXT UNIQUE NOT NULL,       -- ID √∫nico del lote
    month_year TEXT NOT NULL,            -- Per√≠odo (Sep25)
    execution_number INTEGER NOT NULL,   -- N√∫mero de ejecuci√≥n
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    total_tasks INTEGER,                 -- Total de tareas
    completed_tasks INTEGER DEFAULT 0,   -- Tareas completadas
    failed_tasks INTEGER DEFAULT 0,      -- Tareas fallidas
    status TEXT DEFAULT 'running'        -- Estado del lote
);

-- √çndices para optimizaci√≥n
CREATE INDEX idx_tasks_status ON scraping_tasks(status);
CREATE INDEX idx_tasks_batch ON scraping_tasks(execution_batch);
CREATE INDEX idx_batches_date ON execution_batches(started_at);
```

## üöÄ Instalaci√≥n

### üìã **Requisitos del Sistema**

- **Sistema Operativo**: Ubuntu 24.04 LTS (64-bit)
- **Python**: 3.12 o superior
- **Memoria RAM**: M√≠nimo 4GB, recomendado 8GB
- **Espacio en Disco**: M√≠nimo 2GB libres
- **Conexi√≥n a Internet**: Estable para scraping

### ‚ö° **Instalaci√≥n R√°pida**

```bash
# 1. Clonar o descargar el proyecto
cd "/home/esdata/Documents/GitHub/Esdata_710"

# 2. Configurar entorno Python
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 3. Verificar instalaci√≥n
python3 validate_system.py

# 4. Ejecutar demostraci√≥n
python3 orchestrator.py test
```

### üîß **Instalaci√≥n Manual**

```bash
# 1. Verificar Python
python3 --version

# 2. Crear entorno virtual
python3 -m venv venv
source venv/bin/activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Crear directorios
mkdir -p logs temp backups

# 5. Configurar variables de entorno (opcional)
export SCRAPING_CONFIG_PATH="/home/esdata/Documents/GitHub/Esdata_710/config/config.yaml"

# 6. Inicializar base de datos
python3 orchestrator.py test
```

### üîç **Verificaci√≥n de Instalaci√≥n**

```bash
# Validaci√≥n completa del sistema
python3 validate_system.py

# Test b√°sico de funcionalidad
python3 orchestrator.py test

# Verificar configuraci√≥n
python3 monitor_cli.py system
```

## ‚öôÔ∏è Configuraci√≥n

### üìÑ **Archivo Principal: `config/config.yaml`**

```yaml
# Configuraci√≥n de base de datos
database:
  path: "/home/esdata/Documents/GitHub/Esdata_710/orchestrator.db"
  backup_path: "/home/esdata/Documents/GitHub/Esdata_710/backups"
  backup_retention_days: 30

# Configuraci√≥n de ejecuci√≥n
execution:
  max_parallel_scrapers: 6              # Scrapers simult√°neos
  retry_delay_minutes: 30               # Delay entre reintentos
  execution_interval_days: 15           # Frecuencia de ejecuci√≥n
  rate_limit_delay_seconds: 3           # Delay entre requests
  max_retry_attempts: 3                 # M√°ximo de reintentos
  enable_auto_recovery: true            # Recuperaci√≥n autom√°tica

# Configuraci√≥n por sitio web
websites:
  Inm24:
    priority: 1                         # Orden de ejecuci√≥n
    has_detail_scraper: true            # Tiene scraper de detalle
    rate_limit_seconds: 4               # Rate limiting espec√≠fico
    max_pages_per_session: 100          # L√≠mite de p√°ginas
    user_agent: "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
  
  CyT:
    priority: 2
    has_detail_scraper: false
    rate_limit_seconds: 3
    max_pages_per_session: 150

# Configuraci√≥n de monitoreo
monitoring:
  enable_metrics: true                  # Habilitar m√©tricas
  enable_alerts: true                   # Habilitar alertas
  check_interval_seconds: 300           # Intervalo de verificaci√≥n

# Configuraci√≥n de logging
logging:
  level: "INFO"                         # Nivel de log
  max_file_size_mb: 50                  # Tama√±o m√°ximo de archivo
  backup_count: 3                       # Archivos de respaldo
```

### üåê **Configuraci√≥n de URLs: `urls/`**

Cada scraper tiene su archivo CSV correspondiente:

**Formato est√°ndar:**
```csv
PaginaWeb,Ciudad,Operacion,ProductoPaginaWeb,URL
CyT,Gdl,Ven,Dep,https://www.casasyterrenos.com/jalisco/guadalajara/departamentos/venta
CyT,Zap,Ven,Cas,https://www.casasyterrenos.com/jalisco/zapopan/casas/venta
```

**Archivos de configuraci√≥n:**
- `cyt_urls.csv` ‚Üí CasasyTerrenos
- `inm24_urls.csv` ‚Üí Inmuebles24
- `lam_urls.csv` ‚Üí Lamudi
- `mit_urls.csv` ‚Üí Mitula
- `prop_urls.csv` ‚Üí Propiedades.com
- `tro_urls.csv` ‚Üí Trovit

### üîê **Variables de Entorno (Opcional)**

```bash
# Configuraci√≥n avanzada
export SCRAPING_CONFIG_PATH=/path/to/config.yaml
export SCRAPING_LOG_LEVEL=DEBUG
export SCRAPING_MAX_WORKERS=8
export SCRAPING_RATE_LIMIT=2
```

## üéÆ Uso

### üñ•Ô∏è **Menu Interactivo (Recomendado)**

```bash
# Abrir men√∫ principal
python3 monitor_cli.py status --detailed
```

**Opciones disponibles:**
1. Ver estado del sistema
2. Ejecutar scraping completo
3. Ver historial de ejecuciones
4. Ver informaci√≥n del sistema
5. Ver estad√≠sticas
6. Test del sistema
7. Abrir carpeta de datos
8. Editar configuraci√≥n

### üíª **L√≠nea de Comandos**

#### **Orquestador Principal**
```bash
# Ejecutar lote completo de scraping
python3 orchestrator.py run

# Ver estado actual (formato JSON)
python3 orchestrator.py status

# Crear archivos de ejemplo
python3 orchestrator.py setup

# Test b√°sico del sistema
python3 orchestrator.py test

# Ingerir todos los CSV existentes (hist√≥ricos) a tablas SQL
python3 orchestrator.py ingest-existing
```

#### **Monitor CLI**
```bash
# Estado actual del sistema
python3 monitor_cli.py status
python3 monitor_cli.py status --detailed

# Historial de ejecuciones
python3 monitor_cli.py history
python3 monitor_cli.py history --limit 20

# Tareas del √∫ltimo lote
python3 monitor_cli.py tasks
python3 monitor_cli.py tasks --batch-id Sep25_01

# Informaci√≥n del sistema
python3 monitor_cli.py system

# Estad√≠sticas de rendimiento
python3 monitor_cli.py stats
python3 monitor_cli.py stats --days 60

# Ejecutar scraping inmediatamente
python3 monitor_cli.py run
```

#### **Validaci√≥n del Sistema**
```bash
# Validaci√≥n completa
python3 validate_system.py

# Validaci√≥n espec√≠fica
python3 validate_system.py --check dependencies
python3 validate_system.py --check configuration
python3 validate_system.py --check scrapers
```

### üîÑ **Flujo de Trabajo T√≠pico**

#### **1. Preparaci√≥n (Una vez)**
```bash
# Configurar entorno
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Personalizar URLs
nano urls/cyt_urls.csv
nano urls/inm24_urls.csv
# ... editar otros archivos

# Ajustar configuraci√≥n
nano config/config.yaml
```

#### **2. Ejecuci√≥n Regular**
```bash
# Opci√≥n A: Menu interactivo
python3 monitor_cli.py status --detailed

# Opci√≥n B: Comando directo
python3 orchestrator.py run

# Opci√≥n C: Monitoreo continuo
python3 monitor_cli.py status --detailed
```

#### **3. An√°lisis de Resultados**
```bash
# Ver resultados
python3 monitor_cli.py history

# Estad√≠sticas
python3 monitor_cli.py stats

# Abrir datos
xdg-open data
```

## üìä Monitoreo

## üóÑÔ∏è Almacenamiento de Datos SQL

Cada scraper produce un CSV con columnas propias (heterog√©neas). No se fuerza un esquema unificado en una sola tabla; en su lugar:

### Estrategia
- Una tabla por scraper: `data_<scraper>` (configurable con `data_storage.table_prefix`).
- Columnas se crean din√°micamente en la primera ingesti√≥n.
- Si en ejecuciones futuras aparecen nuevas columnas, se a√±aden con `ALTER TABLE` (si `add_missing_columns: true`).
- Los tipos se almacenan inicialmente como `TEXT` para flexibilidad (futura detecci√≥n opcional de tipos).
- Se registra cada archivo ingerido para evitar duplicados (`ingested_files`).
- Se mantiene metadata de columnas por scraper en `scraper_metadata` (JSON simple).

### Tablas Nuevas
```sql
CREATE TABLE ingested_files (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  scraper_name TEXT NOT NULL,
  website TEXT,
  source_file TEXT NOT NULL,
  table_name TEXT NOT NULL,
  rows_ingested INTEGER,
  ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  batch_id TEXT,
  UNIQUE(scraper_name, source_file)
);

CREATE TABLE scraper_metadata (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  scraper_name TEXT NOT NULL UNIQUE,
  table_name TEXT NOT NULL,
  columns_json TEXT NOT NULL,
  last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Configuraci√≥n (`config/config.yaml`)
```yaml
data_storage:
  enable_sql_ingest: true
  table_prefix: "data_"
  normalize_column_names: true
  add_missing_columns: true
  chunk_size: 1000
  track_ingested_files: true
  skip_if_exists: true
  store_metadata: true
  enforce_primary_key: false
  date_detection: false
```

### Flujo Autom√°tico
1. Scraper termina con √©xito ‚Üí se llama a `store_scraper_output`.
2. Se lee el CSV, normaliza columnas (lowercase, underscores, sin espacios).
3. Se crea la tabla si no existe (`data_<scraper>`).
4. Se agregan columnas nuevas detectadas.
5. Se insertan filas en chunks (`chunk_size`).
6. Se actualiza metadata (`scraper_metadata`).
7. Se marca archivo como ingerido (`ingested_files`).

### Reingesti√≥n Manual de Hist√≥ricos
```bash
python3 orchestrator.py ingest-existing
```
Este comando recorre `data/` y trata de inferir el `scraper_name` del nombre del archivo (prefijo antes del primer `_`). √ötil tras activar la funci√≥n de ingesti√≥n por primera vez.

### Consultas R√°pidas
```sql
-- Ver tablas de datos
.tables data_

-- √öltimos archivos ingeridos
SELECT scraper_name, source_file, rows_ingested, ingested_at FROM ingested_files ORDER BY ingested_at DESC LIMIT 20;

-- Columnas registradas para un scraper
SELECT columns_json FROM scraper_metadata WHERE scraper_name='cyt';
```

### Buenas Pr√°cticas
- Mant√©n los CSV como fuente de verdad hist√≥rica (no borrar inmediatamente).
- Para an√°lisis cruzado crea vistas materializadas posteriores con un subconjunto com√∫n de campos.
- Si un scraper empieza a producir IDs estables, puede activarse en el futuro `enforce_primary_key` para a√±adir √≠ndices.

### Pr√≥ximas Extensiones (Roadmap)
- Detecci√≥n de tipos (enteros, fechas) y migraci√≥n suave.
- √çndices configurables por scraper (ej. `precio`, `ciudad`).
- Compresi√≥n autom√°tica de CSV tras ingesti√≥n exitosa.

Si necesitas normalizaci√≥n unificada (wide table) se puede dise√±ar un ETL secundario sin tocar la ingesti√≥n bruta.

### üîÑ Ejemplo Pipeline Dual (Inm24)

1. Fase URL (scraper `inm24.py` en modo URL):
  - El orquestador ejecuta `inm24` y crea un archivo con patr√≥n: `Inm24URL_<Ciudad>_<Operacion>_<Producto>_<MesA√±o>_<Exec>.csv`.
  - Columnas m√≠nimas: `source_scraper,website,city,operation,product,listing_url,collected_at`.
2. Fase Detalle (scraper `inm24_det.py`):
  - Generado din√°micamente solo si el sitio tiene `has_detail_scraper: true`.
  - El adapter inyecta `SCRAPER_URL_LIST_FILE` apuntando al archivo *_URL_*.
  - `inm24_det.py` lee `listing_url` y produce archivo final `Inm24_<Ciudad>_<Operacion>_<Producto>_<MesA√±o>_<Exec>.csv` enriquecido.
3. Ingesti√≥n SQL:
  - Ambos archivos (URL y detalle) pueden ingerirse: se crean tablas `data_inm24` (para detalle) y tambi√©n se puede crear `data_inm24url` si se desea separar (actualmente se usa misma convenci√≥n por scraper principal; para diferenciar podr√≠a a√±adirse un postfijo futuro).
4. Requisitos de robustez:
  - Si falta el archivo *_URL_* el orquestador marca la tarea detalle como FAILED antes de ejecutar.
  - `inm24_det.py` crea placeholder si no encuentra lista (salida suave controlada).

Variables de entorno relevantes (inyectadas):
```
SCRAPER_MODE=url|detail
SCRAPER_OUTPUT_FILE=/ruta/al/archivo/final.csv
SCRAPER_WEBSITE=Inm24
SCRAPER_CITY=Gdl
SCRAPER_OPERATION=Ven
SCRAPER_PRODUCT=Dep
SCRAPER_BATCH_ID=Sep25_01
SCRAPER_URL_LIST_FILE=/ruta/al/archivo/Inm24URL_Gdl_Ven_Dep_Sep25_01.csv (solo en modo detalle)
```

Esto facilita extender a `Lam` repitiendo el mismo patr√≥n sin reescribir orquestaci√≥n.

### üì• Fuente de Tareas: Archivos *_urls.csv

Cada archivo `<scraper>_urls.csv` en la carpeta `urls/` define expl√≠citamente las combinaciones a ejecutar:

Columnas requeridas:
```
PaginaWeb,Ciudad,Operacion,ProductoPaginaWeb,URL
```
Ejemplo (`inm24_urls.csv`):
```
PaginaWeb,Ciudad,Operacion,ProductoPaginaWeb,URL
Inm24,Gdl,Ven,Dep,https://www.inmuebles24.com/departamentos-en-venta-en-pagina-1.html
Inm24,Zap,Ven,Dep,https://www.inmuebles24.com/departamentos-en-venta-en-zapopan-pagina-1.html
```

Para cada fila el orquestador crea una tarea y pasa la columna `URL` al adaptador como `SCRAPER_INPUT_URL`. El scraper en modo URL genera el archivo *_URL_* aplicando paginaci√≥n incremental: reemplaza `pagina-<n>` o a√±ade sufijos si corresponde, deteni√©ndose cuando no aparecen nuevos enlaces.

Beneficios de este modelo:
- Control expl√≠cito de combinaciones (evita l√≥gica harcodeada dentro del scraper).
- F√°cil activar/desactivar ciudades modificando un CSV.
- Escalable a m√°s productos/operaciones sin cambiar c√≥digo.

Si en un futuro se requiere parametrizar el n√∫mero m√°ximo de p√°ginas por fila, se puede a√±adir una columna opcional `MaxPaginas` y el adaptador podr√≠a exponerla como variable de entorno adicional.

### üéØ **Dashboard en Tiempo Real**

```bash
# Dashboard interactivo
python3 monitor_cli.py status --detailed
```

**Informaci√≥n mostrada:**
- Estado actual del sistema
- Progreso de lotes en ejecuci√≥n
- Tareas completadas/fallidas/pendientes
- Tiempo de ejecuci√≥n
- Errores recientes

### üìà **M√©tricas del Sistema**

#### **M√©tricas de Rendimiento**
- **Tasa de √©xito**: Porcentaje de tareas completadas exitosamente
- **Tiempo promedio**: Duraci√≥n media de ejecuci√≥n por scraper
- **Throughput**: N√∫mero de p√°ginas procesadas por minuto
- **Volumen de datos**: Cantidad de registros extra√≠dos

#### **M√©tricas de Calidad**
- **Reintentos**: N√∫mero promedio de reintentos por tarea
- **Errores**: Tipos y frecuencia de errores
- **Cobertura**: Porcentaje de URLs procesadas exitosamente

#### **M√©tricas del Sistema**
- **Uso de CPU**: Porcentaje de utilizaci√≥n
- **Uso de memoria**: RAM consumida por el sistema
- **Espacio en disco**: Almacenamiento utilizado
- **Conexiones de red**: Estado de conectividad

### üö® **Sistema de Alertas**

#### **Alertas Autom√°ticas**
- **Fallo cr√≠tico**: M√°s del 50% de scrapers fallando
- **Tiempo excedido**: Ejecuci√≥n tomando m√°s de 2 horas
- **Espacio insuficiente**: Menos de 1GB de espacio libre
- **Error de conectividad**: Problemas de red persistentes

#### **Configuraci√≥n de Alertas**
```yaml
# En config.yaml
monitoring:
  alerts:
    email:
      enabled: true
      smtp_server: "smtp.gmail.com"
      recipients: ["admin@empresa.com"]
    
    slack:
      enabled: false
      webhook_url: "https://hooks.slack.com/..."
    
    thresholds:
      failure_rate: 0.5              # 50% de fallos
      execution_time_hours: 2        # 2 horas m√°ximo
      disk_space_gb: 1               # 1GB m√≠nimo
```

### üìã **Reportes Autom√°ticos**

#### **Reporte Diario**
```bash
# Generar reporte del d√≠a
python3 monitor_cli.py stats --days 1 > reports/daily_$(date +%Y%m%d).txt
```

#### **Reporte Semanal**
```bash
# Generar reporte semanal
python3 monitor_cli.py stats --days 7 > reports/weekly_$(date +%Y%m%d).txt
```

#### **Reporte de Salud del Sistema**
```bash
# Generar reporte completo
python3 validate_system.py > reports/health_$(date +%Y%m%d).txt
```

## üîå API y CLI

### üñ•Ô∏è **Interfaz de L√≠nea de Comandos (CLI)**

#### **Orchestrator CLI**
```bash
# Sintaxis general
python3 orchestrator.py <comando> [opciones]

# Comandos disponibles
Commands:
  run      Ejecutar lote completo de scraping
  status   Mostrar estado actual (JSON)
  setup    Crear archivos de configuraci√≥n ejemplo
  test     Ejecutar test b√°sico del sistema

# Ejemplos
python3 orchestrator.py run
python3 orchestrator.py status | jq '.last_batch'
python3 orchestrator.py test --verbose
```

#### **Monitor CLI**
```bash
# Sintaxis general
python3 monitor_cli.py <comando> [opciones]

# Comandos de estado
monitor_cli.py status [--detailed]
monitor_cli.py system
monitor_cli.py tasks [--batch-id BATCH]

# Comandos de historial
monitor_cli.py history [--limit N]
monitor_cli.py stats [--days N] [--website SITE]

# Comandos de control
monitor_cli.py run
```

### üì° **API Interna (Para Desarrollo)**

#### **Orchestrator API**
```python
from orchestrator import WindowsScrapingOrchestrator

# Crear instancia
orch = WindowsScrapingOrchestrator()

# Ejecutar scraping
success = orch.run_execution_batch()

# Obtener estado
status = orch.get_status_report()

# Configurar scraper espec√≠fico
task_info = {
    'scraper_name': 'cyt',
    'website': 'CyT',
    'city': 'Gdl',
    'operation': 'Ven',
    'product': 'Dep',
    'url': 'https://example.com'
}
```

#### **Monitor API**
```python
from monitor_cli import ScrapingMonitorWindows

# Crear monitor
monitor = ScrapingMonitorWindows()

# Mostrar estado
monitor.show_status(detailed=True)

# Mostrar estad√≠sticas
monitor.show_stats(days=30)

# Ejecutar scraping
monitor.run_now()
```

#### **Database API**
```python
import sqlite3
from pathlib import Path

# Conectar a base de datos
db_path = Path("orchestrator.db")
conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row

# Consultar tareas
cursor = conn.execute("""
    SELECT * FROM scraping_tasks 
    WHERE status = 'completed' 
    ORDER BY completed_at DESC
""")
tasks = cursor.fetchall()

# Consultar estad√≠sticas
cursor = conn.execute("""
    SELECT website, COUNT(*) as total,
           SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as success
    FROM scraping_tasks
    GROUP BY website
""")
stats = cursor.fetchall()
```

### üîß **C√≥digos de Salida**

| C√≥digo | Significado | Acci√≥n |
|--------|-------------|---------|
| 0 | √âxito | Operaci√≥n completada correctamente |
| 1 | Error general | Verificar logs y configuraci√≥n |
| 2 | Error de configuraci√≥n | Revisar config.yaml |
| 3 | Error de dependencias | Ejecutar `pip install -r requirements.txt` |
| 4 | Error de base de datos | Verificar permisos y espacio |
| 5 | Error de red | Verificar conectividad |

## üõ†Ô∏è Desarrollo

### üèóÔ∏è **Arquitectura de Desarrollo**

#### **Estructura de M√≥dulos**
```
src/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py          # Motor principal
‚îÇ   ‚îú‚îÄ‚îÄ task_manager.py          # Gesti√≥n de tareas
‚îÇ   ‚îú‚îÄ‚îÄ database.py              # Abstracci√≥n de DB
‚îÇ   ‚îî‚îÄ‚îÄ config.py                # Gesti√≥n de configuraci√≥n
‚îú‚îÄ‚îÄ adapters/
‚îÇ   ‚îú‚îÄ‚îÄ scraper_adapter.py       # Adaptador base
‚îÇ   ‚îú‚îÄ‚îÄ cyt_adapter.py           # Adaptador espec√≠fico CyT
‚îÇ   ‚îî‚îÄ‚îÄ inm24_adapter.py         # Adaptador espec√≠fico Inm24
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py               # Recolecci√≥n de m√©tricas
‚îÇ   ‚îú‚îÄ‚îÄ alerting.py              # Sistema de alertas
‚îÇ   ‚îî‚îÄ‚îÄ dashboard.py             # Dashboard web (futuro)
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ logging.py               # Utilidades de logging
‚îÇ   ‚îú‚îÄ‚îÄ validation.py            # Validaciones
‚îÇ   ‚îî‚îÄ‚îÄ helpers.py               # Funciones auxiliares
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ unit/                    # Tests unitarios
    ‚îú‚îÄ‚îÄ integration/             # Tests de integraci√≥n
    ‚îî‚îÄ‚îÄ e2e/                     # Tests end-to-end
```

### üß™ **Testing**

#### **Tests Unitarios**
```python
# tests/unit/test_orchestrator.py
import unittest
from unittest.mock import Mock, patch
from core.orchestrator import WindowsScrapingOrchestrator

class TestOrchestrator(unittest.TestCase):
    def setUp(self):
        self.orchestrator = WindowsScrapingOrchestrator()
    
    def test_generate_batch_id(self):
        batch_id, month_year, exec_num = self.orchestrator.generate_batch_id()
        self.assertRegex(batch_id, r'^[A-Z][a-z]{2}\d{2}_\d{2}$')
    
    def test_load_urls_from_csv(self):
        # Mock CSV file
        with patch('pandas.read_csv') as mock_csv:
            mock_csv.return_value = Mock()
            tasks = self.orchestrator.load_urls_from_csv('cyt')
            self.assertIsInstance(tasks, list)

# Ejecutar tests
python -m pytest tests/unit/ -v
```

#### **Tests de Integraci√≥n**
```python
# tests/integration/test_scraper_integration.py
import unittest
from pathlib import Path
from improved_scraper_adapter import ImprovedScraperAdapter

class TestScraperIntegration(unittest.TestCase):
    def setUp(self):
        self.base_dir = Path("test_data")
        self.adapter = ImprovedScraperAdapter(self.base_dir)
    
    def test_cyt_scraper_adaptation(self):
        task_info = {
            'scraper_name': 'cyt',
            'website': 'CyT',
            'city': 'Gdl',
            'operation': 'Ven',
            'product': 'Dep',
            'url': 'https://test.com',
            'output_file': 'test_output.csv'
        }
        
        result = self.adapter.adapt_and_execute_scraper(task_info)
        self.assertTrue(result)

# Ejecutar tests
python -m pytest tests/integration/ -v
```

#### **Tests End-to-End**
```python
# tests/e2e/test_complete_workflow.py
import unittest
import subprocess
from pathlib import Path

class TestCompleteWorkflow(unittest.TestCase):
    def test_complete_scraping_workflow(self):
        # Test setup ‚Üí validation ‚Üí execution ‚Üí monitoring
        
        # 1. Setup
        result = subprocess.run(['python', 'orchestrator.py', 'setup'], 
                              capture_output=True, text=True)
        self.assertEqual(result.returncode, 0)
        
        # 2. Validation
        result = subprocess.run(['python', 'validate_system.py'], 
                              capture_output=True, text=True)
        self.assertEqual(result.returncode, 0)
        
        # 3. Execution
        result = subprocess.run(['python', 'orchestrator.py', 'run'], 
                              capture_output=True, text=True)
        self.assertEqual(result.returncode, 0)
        
        # 4. Monitoring
        result = subprocess.run(['python', 'monitor_cli.py', 'status'], 
                              capture_output=True, text=True)
        self.assertEqual(result.returncode, 0)

# Ejecutar tests
python -m pytest tests/e2e/ -v --slow
```

### üîß **Extensi√≥n del Sistema**

#### **Agregar Nuevo Sitio Web**

**1. Crear Scraper:**
```python
# Scrapers/nuevo_sitio.py
import pandas as pd
from bs4 import BeautifulSoup
import requests

def scrape_page_source(html):
    # L√≥gica de extracci√≥n espec√≠fica
    pass

def main():
    # L√≥gica principal del scraper
    pass

if __name__ == "__main__":
    main()
```

**2. Configurar en YAML:**
```yaml
# config/config.yaml
websites:
  NuevoSitio:
    priority: 7
    has_detail_scraper: false
    rate_limit_seconds: 2
    max_pages_per_session: 50
```

**3. Crear URLs:**
```csv
# urls/nuevo_sitio_urls.csv
PaginaWeb,Ciudad,Operacion,ProductoPaginaWeb,URL
NuevoSitio,Gdl,Ven,Dep,https://nuevo-sitio.com/guadalajara/departamentos
```

**4. Registrar en Adaptador:**
```python
# improved_scraper_adapter.py
self.scraper_configs['nuevo_sitio'] = {
    'has_main': True,
    'url_parameter': 'URL_BASE',
    'output_method': 'save',
    'needs_url_modification': True
}
```

#### **Agregar Nueva Funcionalidad de Monitoreo**

```python
# monitoring/custom_metrics.py
class CustomMetricsCollector:
    def collect_custom_metrics(self):
        """Recolectar m√©tricas personalizadas"""
        metrics = {}
        
        # Ejemplo: M√©trica de velocidad de scraping
        metrics['pages_per_minute'] = self.calculate_scraping_speed()
        
        # Ejemplo: M√©trica de calidad de datos
        metrics['data_quality_score'] = self.calculate_data_quality()
        
        return metrics
    
    def calculate_scraping_speed(self):
        # Implementar c√°lculo
        pass
    
    def calculate_data_quality(self):
        # Implementar c√°lculo
        pass
```

### üìù **Gu√≠as de Contribuci√≥n**

#### **Est√°ndares de C√≥digo**
- **PEP 8**: Seguir est√°ndares de estilo Python
- **Type Hints**: Usar anotaciones de tipo
- **Docstrings**: Documentar todas las funciones p√∫blicas
- **Testing**: Cobertura m√≠nima del 80%

#### **Proceso de Desarrollo**
1. **Fork** del repositorio
2. **Crear rama** para la funcionalidad: `git checkout -b feature/nueva-funcionalidad`
3. **Desarrollar** con tests incluidos
4. **Ejecutar tests**: `python -m pytest`
5. **Validar c√≥digo**: `python3 validate_system.py`
6. **Commit** con mensaje descriptivo
7. **Push** y crear **Pull Request**

## üîß Troubleshooting

### üö® **Problemas Comunes**

#### **Error: "Base de datos no encontrada"**
```
S√≠ntoma: SQLite database not found
Causa: Primera ejecuci√≥n o base de datos corrupta
Soluci√≥n:
1. python3 orchestrator.py run    # Crea DB autom√°ticamente
2. Verificar permisos de escritura en directorio
3. Si persiste: eliminar orchestrator.db y re-ejecutar
```

#### **Error: "Scraper no encontrado"**
```
S√≠ntoma: FileNotFoundError: Scraper not found
Causa: Archivo .py faltante o ruta incorrecta
Soluci√≥n:
1. Verificar existencia: python3 monitor_cli.py system
2. Validar estructura: dir Scrapers\
3. Re-descargar scrapers faltantes
```

#### **Error: "Dependencias faltantes"**
```
S√≠ntoma: ModuleNotFoundError: No module named 'X'
Causa: Librer√≠as Python no instaladas
Soluci√≥n:
1. pip install -r requirements.txt
2. python -m pip install --upgrade pip
3. Si persiste: crear nuevo entorno virtual
```

#### **Error: "Archivo CSV no v√°lido"**
```
S√≠ntoma: CSV parsing error o columnas faltantes
Causa: Formato incorrecto en archivos de URLs
Soluci√≥n:
1. Verificar formato: usar Excel o editor de texto
2. Validar columnas requeridas:
   - PaginaWeb, Ciudad, Operacion, ProductoPaginaWeb, URL
3. Recrear archivos: python3 orchestrator.py setup
```

#### **Error: "Chrome driver not found"**
```
S√≠ntoma: WebDriverException: chromedriver not found
Causa: ChromeDriver no instalado o desactualizado
Soluci√≥n:
1. Instalar Chrome: wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add - && sudo apt-get update && sudo apt-get install google-chrome-stable
2. Instalar ChromeDriver: sudo apt-get install chromium-chromedriver
3. Alternativa: usar seleniumbase auto-download
4. Verificar instalaci√≥n: which chromedriver
```

#### **Error: "Rate limit exceeded"**
```
S√≠ntoma: Too many requests o 429 HTTP error
Causa: Scraping demasiado r√°pido
Soluci√≥n:
1. Aumentar delay: config.yaml ‚Üí rate_limit_delay_seconds
2. Reducir scrapers paralelos: max_parallel_scrapers
3. Verificar robots.txt del sitio web
```

#### **Error: "Timeout en scraper"**
```
S√≠ntoma: Process timeout o hung scraper
Causa: P√°gina web lenta o problema de red
Soluci√≥n:
1. Aumentar timeout: config.yaml ‚Üí timeout_minutes
2. Verificar conectividad: ping sitio-web.com
3. Revisar logs: type logs\orchestrator.log
```

#### **Error: "Espacio insuficiente"**
```
S√≠ntoma: Disk space error o cannot write file
Causa: Disco lleno o permisos insuficientes
Soluci√≥n:
1. Liberar espacio: ejecutar cleanup
2. Verificar permisos de escritura
3. Cambiar ubicaci√≥n: editar paths en config.yaml
```

### ü©∫ **Diagn√≥stico Avanzado**

#### **Diagn√≥stico Completo del Sistema**
```bash
# Ejecutar diagn√≥stico completo
python3 validate_system.py > diagnostico.txt 2>&1

# Verificar logs detallados
python3 monitor_cli.py system

# Test de conectividad
ping www.inmuebles24.com
ping www.casasyterrenos.com

# Verificar recursos del sistema
python -c "import psutil; print(f'CPU: {psutil.cpu_percent()}%, RAM: {psutil.virtual_memory().percent}%')"
```

#### **Logs de Debug**
```bash
# Activar logging detallado
export PYTHONPATH="$PYTHONPATH:/home/esdata/Documents/GitHub/Esdata_710"
python -c "
import logging
logging.basicConfig(level=logging.DEBUG)
from orchestrator import WindowsScrapingOrchestrator
orch = WindowsScrapingOrchestrator()
orch.run_execution_batch()
"

# Ver logs en tiempo real
tail -f logs/orchestrator.log
```

#### **An√°lisis de Performance**
```python
# performance_analyzer.py
import time
import psutil
from pathlib import Path

def analyze_performance():
    start_time = time.time()
    start_memory = psutil.virtual_memory().used
    
    # Ejecutar operaci√≥n
    import subprocess
    result = subprocess.run(['python', 'orchestrator.py', 'run'], 
                          capture_output=True, text=True)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().used
    
    print(f"Tiempo de ejecuci√≥n: {end_time - start_time:.2f} segundos")
    print(f"Memoria utilizada: {(end_memory - start_memory) / 1024**2:.2f} MB")
    print(f"C√≥digo de salida: {result.returncode}")

if __name__ == "__main__":
    analyze_performance()
```

### üÜò **Recuperaci√≥n de Emergencia**

#### **Restaurar desde Backup**
```bash
# Ubicar √∫ltimo backup
ls -la backups/

# Restaurar base de datos
cp backups/scraping_backup_YYYYMMDD_HHMMSS.tar.gz .
tar -xzf scraping_backup_YYYYMMDD_HHMMSS.tar.gz
cp scraping_backup_YYYYMMDD_HHMMSS/orchestrator.db .

# Verificar restauraci√≥n
python3 monitor_cli.py status
```

#### **Reset Completo del Sistema**
```bash
# CUIDADO: Esto eliminar√° todos los datos
echo "Esta operaci√≥n eliminar√° todos los datos. ¬øContinuar? (y/N)"
read confirm
if [ "$confirm" = "y" ] || [ "$confirm" = "Y" ]; then
    rm -rf data logs temp backups
    rm -f orchestrator.db
    python3 -m venv venv
    source venv/bin/activate
    pip install -r requirements.txt
    echo "Sistema reseteado completamente"
fi
```

#### **Modo de Emergencia (Solo Monitoreo)**
```bash
# Ejecutar solo monitoreo sin scraping
python3 monitor_cli.py status
python3 monitor_cli.py system
python3 monitor_cli.py history --limit 10

# Verificar integridad de datos
python -c "
import sqlite3
conn = sqlite3.connect('orchestrator.db')
cursor = conn.execute('PRAGMA integrity_check')
print(cursor.fetchall())
"
```

### üìû **Soporte T√©cnico**

#### **Informaci√≥n para Reportar Problemas**
```bash
# Generar reporte completo para soporte
echo "=== REPORTE DE SOPORTE ===" > soporte.txt
echo "Fecha: $(date)" >> soporte.txt
echo "" >> soporte.txt

echo "=== INFORMACI√ìN DEL SISTEMA ===" >> soporte.txt
lsb_release -a >> soporte.txt 2>/dev/null
python3 --version >> soporte.txt
echo "" >> soporte.txt

echo "=== VALIDACI√ìN DEL SISTEMA ===" >> soporte.txt
python3 validate_system.py >> soporte.txt 2>&1
echo "" >> soporte.txt

echo "=== √öLTIMOS LOGS ===" >> soporte.txt
grep -E "ERROR|CRITICAL" logs/orchestrator.log | tail -20 >> soporte.txt
echo "" >> soporte.txt

echo "=== ESTADO ACTUAL ===" >> soporte.txt
python3 monitor_cli.py status >> soporte.txt 2>&1

echo "Reporte generado en soporte.txt"
```

#### **Contacto de Soporte**
- **Email**: soporte@scraping-system.com
- **Documentaci√≥n**: [docs.scraping-system.com](https://docs.scraping-system.com)
- **GitHub Issues**: [github.com/proyecto/issues](https://github.com/proyecto/issues)
- **Slack**: #soporte-scraping

## üìö Recursos Adicionales

### üìñ **Documentaci√≥n T√©cnica**
- [Gu√≠a de Arquitectura](docs/ARCHITECTURE.md)
- [API Reference](docs/API.md)
- [Database Schema](docs/DATABASE.md)
- [Deployment Guide](docs/DEPLOYMENT.md)

### üéì **Tutoriales**
- [Tutorial: Primer Scraping](docs/tutorials/FIRST_SCRAPING.md)
- [Tutorial: Agregar Sitio Web](docs/tutorials/ADD_WEBSITE.md)
- [Tutorial: Configuraci√≥n Avanzada](docs/tutorials/ADVANCED_CONFIG.md)

### üîó **Enlaces √ötiles**
- [Python Official Documentation](https://docs.python.org/)
- [Selenium Documentation](https://selenium-python.readthedocs.io/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)

## üìÑ Licencia

Este proyecto est√° licenciado bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

```
MIT License

Copyright (c) 2025 Sistema de Orquestaci√≥n de Scraping Inmobiliario

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## ü§ù Contribuci√≥n

¬°Las contribuciones son bienvenidas! Por favor lee [CONTRIBUTING.md](CONTRIBUTING.md) para conocer nuestro c√≥digo de conducta y el proceso para enviar pull requests.

### üë• **Colaboradores**

- **Desarrollador Principal**: [Tu Nombre](mailto:tu@email.com)
- **Arquitecto de Sistema**: [Colaborador](mailto:colaborador@email.com)
- **Especialista en Scraping**: [Especialista](mailto:especialista@email.com)

### üôè **Agradecimientos**

- Comunidad Python por las excelentes librer√≠as
- Selenium y BeautifulSoup por hacer posible el web scraping
- Todos los contribuidores que han ayudado a mejorar este proyecto

---

**¬øNecesitas ayuda?** 
- üìß Email: soporte@scraping-system.com
- üí¨ Slack: #soporte-scraping
- üìñ Documentaci√≥n: [docs.scraping-system.com](https://docs.scraping-system.com)

**¬°Feliz Scraping! üöÄ**
