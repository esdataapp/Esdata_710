# Sistema de OrquestaciÃ³n de Scraping Inmobiliario

Este repositorio contiene una plataforma completa para ejecutar y monitorear scrapers inmobiliarios en Ubuntu 24.04. La nueva
arquitectura estÃ¡ centrada en un orquestador asÃ­ncrono que coordina los scrapers principales y de detalle a partir de los CSV de
la carpeta `urls/`, registra el estado en SQLite y organiza los datos en un Ã¡rbol jerÃ¡rquico listo para anÃ¡lisis.

## ğŸ“¦ Componentes principales

| Componente | DescripciÃ³n |
|------------|-------------|
| `orchestrator.py` | CLI principal. Carga los CSV, genera lotes y ejecuta los scrapers respetando dependencias y prioridades. |
| `esdata/` | Paquete con la lÃ³gica interna (configuraciÃ³n, base de datos, scheduler, normalizaciÃ³n de variables). |
| `scraper_adapter.py` | Adaptador que expone una interfaz uniforme para los scrapers existentes y valida sus salidas. |
| `monitor_cli.py` | CLI ligera para consultar estados de lotes y tareas desde la terminal de Linux. |
| `validate_system.py` | Verificador rÃ¡pido para confirmar estructura de carpetas, CSV, scrapers y esquema de base de datos. |
| `Scrapers/` | Scrapers principales y de detalle. Reciben parÃ¡metros mediante variables de entorno. |
| `urls/` | CSV de entrada que definen `PaginaWeb`, `Ciudad`, `Operacion`, `ProductoPaginaWeb` y `URL`.

## ğŸ—‚ï¸ Estructura de directorios

```
Esdata_710/
â”œâ”€â”€ config/                # config.yaml y ajustes adicionales
â”œâ”€â”€ data/                  # Salidas de scraping organizadas por sitio/ciudad/operaciÃ³n/producto/mes/lote
â”œâ”€â”€ logs/                  # Registros del orquestador y utilidades
â”œâ”€â”€ Scrapers/              # Scrapers principales y de detalle
â”œâ”€â”€ urls/                  # CSV de orquestaciÃ³n (un archivo por scraper)
â”œâ”€â”€ esdata/                # CÃ³digo fuente de la nueva arquitectura
â”œâ”€â”€ orchestrator.db        # Base de datos SQLite con tareas y lotes
â”œâ”€â”€ orchestrator.py        # Orquestador CLI
â”œâ”€â”€ monitor_cli.py         # Monitor en terminal
â”œâ”€â”€ scraper_adapter.py     # Adaptador de scrapers
â””â”€â”€ validate_system.py     # Validador de instalaciÃ³n
```

## âš™ï¸ Requisitos

- Python 3.12 o superior
- Paquetes listados en `requirements.txt`
- Google Chrome/Chromium y controlador compatible (para scrapers Selenium)

InstalaciÃ³n de dependencias:

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

## ğŸ§¾ ConfiguraciÃ³n

La configuraciÃ³n central se encuentra en `config/config.yaml`. Los valores mÃ¡s relevantes son:

- `database.path`: ruta (relativa al proyecto) del archivo SQLite.
- `data.base_path`: carpeta raÃ­z donde se guardan los CSV generados.
- `data.urls_path`: carpeta con los CSV de orquestaciÃ³n.
- `execution.max_parallel_scrapers`: nÃºmero mÃ¡ximo de scrapers simultÃ¡neos.
- `execution.max_retry_attempts`: intentos por tarea antes de marcarla como fallida.
- `websites`: metadatos por sitio (prioridad, scraper de detalle, rate limiting, etc.).
- `aliases`: normalizaciÃ³n de nombres leÃ­dos desde los CSV.

El archivo `Lista de Variables/Lista de Variables Orquestacion.csv` define abreviaturas oficiales para sitios, ciudades, operaciones y productos. El orquestador las carga automÃ¡ticamente.

## â–¶ï¸ EjecuciÃ³n del orquestador

1. **Planificar** sin ejecutar:
   ```bash
   python orchestrator.py plan
   ```
   Muestra cuÃ¡ntas tareas se generarÃ¡n por scraper y cuÃ¡ntas son de detalle.

2. **Ejecutar** un lote completo:
   ```bash
   python orchestrator.py run
   ```
   - Lee todos los CSV de `urls/`.
   - Genera un lote con identificador `<Mes><AÃ±o>_<01|02>` segÃºn la quincena.
   - Ejecuta los scrapers respetando prioridades (Inm24 siempre activo + rotaciÃ³n del resto) y libera automÃ¡ticamente los scrapers de detalle.
   - Registra cada tarea en `orchestrator.db` con estados `pending`, `running`, `retrying`, `completed`, `failed` o `blocked`.

3. **Reanudar** un lote interrumpido:
   ```bash
   python orchestrator.py resume
   ```
   Si el lote anterior quedÃ³ en estado `running`, el scheduler retomarÃ¡ las tareas pendientes.

4. **Restringir scrapers** especÃ­ficos:
   ```bash
   python orchestrator.py run --scrapers inm24 lam
   ```

## ğŸ“Š Monitoreo en terminal

El script `monitor_cli.py` expone comandos simples para consultar el estado sin necesidad de dashboards externos:

```bash
python monitor_cli.py overview         # Resumen del lote activo o Ãºltimo finalizado
python monitor_cli.py batches --limit 5 # Historial de los Ãºltimos lotes
python monitor_cli.py tasks --status pending running --batch Sep25_01
```

Las salidas se presentan en tablas tipo Markdown para copiarlas fÃ¡cilmente a reportes o chats internos.

## âœ… ValidaciÃ³n rÃ¡pida

Antes de poner en marcha el sistema en un servidor limpio, ejecute:

```bash
python validate_system.py
```

El validador comprueba la estructura de carpetas, que los CSV contengan las columnas obligatorias, que los scrapers existan y que la base de datos tenga el esquema mÃ­nimo (`scraping_tasks`, `execution_batches`).

## ğŸ—ƒï¸ OrganizaciÃ³n de datos

Cada tarea genera los archivos en `data/<PaginaWeb>/<Ciudad>/<Operacion>/<Producto>/<MesAÃ±o>/<EjecuciÃ³n>/`. Los scrapers principales producen archivos con sufijo `URL_...`, mientras que los scrapers de detalle escriben en la misma carpeta sin el sufijo. Ejemplo:

```
data/
â””â”€â”€ Inm24/
    â””â”€â”€ Gdl/
        â””â”€â”€ Ven/
            â””â”€â”€ Dep/
                â””â”€â”€ Sep25/
                    â””â”€â”€ 01/
                        â”œâ”€â”€ Inm24URL_Gdl_Ven_Dep_Sep25_01.csv
                        â””â”€â”€ Inm24_Gdl_Ven_Dep_Sep25_01.csv
```

## ğŸ“„ Licencia

Este proyecto se distribuye bajo la licencia MIT. Consulte el archivo `LICENSE` para mÃ¡s informaciÃ³n.
