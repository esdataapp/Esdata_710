# Sistema de OrquestaciÃ³n de Scraping Inmobiliario

La carpeta **Esdata_710** contiene una plataforma lista para coordinar scrapers
inmobiliarios guiados por archivos CSV. La orquestaciÃ³n es asÃ­ncrona, registra
todos los eventos en SQLite y genera los archivos siguiendo la jerarquÃ­a
``<PaginaWeb>/<Ciudad>/<Operacion>/<Producto>/<MesAÃ±o>/<EjecuciÃ³n>``.  
Los scrapers incluidos son **sintÃ©ticos**: producen datos deterministas a partir
de las URLs del CSV para validar el flujo extremo a extremo sin depender de
Selenium ni acceso a internet. En producciÃ³n se pueden sustituir fÃ¡cilmente por
implementaciones reales reutilizando la misma infraestructura.

## ğŸ“¦ Componentes

| Componente | DescripciÃ³n |
|------------|-------------|
| `orchestrator.py` | CLI principal. Planea lotes, ejecuta scrapers en paralelo y gestiona reintentos. |
| `esdata/` | Paquete con la configuraciÃ³n, el repositorio SQLite, el scheduler y utilidades compartidas. |
| `esdata/scrapers/common.py` | API de apoyo para leer contexto desde el orquestador y generar archivos normalizados. |
| `Scrapers/` | Scrapers principales y de detalle basados en `common.py`. Funcionan sin Selenium. |
| `Scrapers Originales/` | CÃ³digo histÃ³rico de referencia. Se conserva pero no participa en la orquestaciÃ³n. |
| `monitor_cli.py` | Monitor en terminal para revisar lotes, tareas y mÃ©tricas rÃ¡pidas. |
| `validate_system.py` | Verificador de instalaciÃ³n (estructura de carpetas, CSV, scrapers y base de datos). |

## ğŸ—‚ï¸ Flujo general

1. Cada scraper tiene un CSV en `urls/` con las columnas obligatorias
   `PaginaWeb`, `Ciudad`, `Operacion`, `ProductoPaginaWeb`, `URL`.
2. El orquestador genera tareas respetando el orden del CSV y crea un lote con
   identificador `<Mes><AÃ±o>_<01|02>` dependiendo de la quincena actual.
3. Los scrapers principales (Inm24, Lam, CyT, Mit, Prop, Tro) producen archivos
   `*URL_...csv` con URLs sintÃ©ticas y los metadatos del lote.
4. Los scrapers de detalle (`inm24_det`, `lam_det`) leen el archivo puente y
   generan informaciÃ³n adicional en el mismo directorio.
5. Todo el progreso queda registrado en `orchestrator.db` para permitir
   reanudaciones y anÃ¡lisis posteriores.

## âš™ï¸ Requisitos

- Python 3.12 o superior.
- Dependencias Python mÃ­nimas definidas en `requirements.txt`:
  `numpy`, `pandas`, `pyyaml`, `tabulate`, `psutil`.

InstalaciÃ³n recomendada:

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

> **Nota:** No se necesita Selenium ni controladores de navegador para ejecutar
> los scrapers incluidos. Para migrar a scraping real basta con sustituir la
> lÃ³gica sintÃ©tica por la extracciÃ³n deseada conservando la llamada a
> `build_context()` y `run_main_scraper()` / `run_detail_scraper()`.

## ğŸ§¾ ConfiguraciÃ³n

La configuraciÃ³n central estÃ¡ en `config/config.yaml`.

- `database.path`: ruta (relativa) del archivo SQLite.
- `data.base_path`: carpeta base de salida (`data/` por defecto).
- `execution.max_parallel_scrapers`: concurrencia mÃ¡xima.
- `execution.max_retry_attempts`: intentos antes de marcar una tarea como fallida.
- `websites`: metadatos por sitio (prioridad, scraper de detalle, lÃ­mites de pÃ¡ginas).
- `aliases`: normalizaciÃ³n de los valores leÃ­dos desde los CSV (abreviaturas oficiales).

El archivo `Lista de Variables/Lista de Variables Orquestacion.csv` define las
abreviaturas que utiliza el sistema (por ejemplo `Inm24`, `Lam`, `Gdl`, `Ven`).

## â–¶ï¸ EjecuciÃ³n bÃ¡sica

1. **Planificar tareas**
   ```bash
   python orchestrator.py plan
   ```

2. **Ejecutar un lote completo**
   ```bash
   python orchestrator.py run
   ```
   - Crea las carpetas destino siguiendo la jerarquÃ­a oficial.
   - Mantiene siempre activo el scraper prioritario (Inm24) y rota el resto
     segÃºn la prioridad configurada.
   - Cuando un scraper principal termina, libera automÃ¡ticamente su scraper de
     detalle asociado.

3. **Reanudar un lote interrumpido**
   ```bash
   python orchestrator.py resume
   ```

4. **Filtrar scrapers especÃ­ficos**
   ```bash
   python orchestrator.py run --scrapers inm24 lam
   ```

## ğŸ“Š Monitoreo y validaciÃ³n

- `python monitor_cli.py overview` â€“ estado general del lote activo o del Ãºltimo
  completado.
- `python monitor_cli.py batches --limit 5` â€“ historial resumido de ejecuciones.
- `python monitor_cli.py tasks --status pending failed` â€“ detalle de tareas por estado.
- `python validate_system.py` â€“ confirma que existan directorios, CSV, scrapers y
  tablas en la base de datos.

## ğŸ—ƒï¸ Datos generados

Los archivos se almacenan en:

```
data/<PaginaWeb>/<Ciudad>/<Operacion>/<Producto>/<MesAÃ±o>/<EjecuciÃ³n>/
```

Ejemplo con datos sintÃ©ticos:

```
data/
â””â”€â”€ Inm24/
    â””â”€â”€ Gdl/
        â””â”€â”€ Ven/
            â””â”€â”€ Dep/
                â””â”€â”€ Sep25/
                    â””â”€â”€ 01/
                        â”œâ”€â”€ Inm24URL_Gdl_Ven_Dep_Sep25_01.csv
                        â””â”€â”€ Inm24_Gdl_Ven_Dep_Sep25_01.csv
```

Los scrapers sintÃ©ticos generan columnas como `listing_url`, `collected_at`,
`detail_id`, `price_hint`, etc. Estas columnas pueden sustituirse por datos
reales sin modificar la orquestaciÃ³n.

## ğŸ› ï¸ Personalizar scrapers reales

1. Importar las utilidades compartidas:
   ```python
   from esdata.scrapers.common import build_context, run_main_scraper
   context = build_context("inm24")
   ```
2. Reemplazar la generaciÃ³n sintÃ©tica por la lÃ³gica real (requests, Selenium,
   etc.) y escribir el CSV en `context.output_file`.
3. Mantener la estructura de columnas esperada (`listing_url`, metadatos del
   contexto) para preservar la compatibilidad con los scrapers de detalle.
4. Para scrapers de detalle utilizar `run_detail_scraper()` como referencia:
   el orquestador inyecta la ruta del archivo puente mediante
   `SCRAPER_URL_LIST_FILE`.

## ğŸš€ PrÃ³ximos pasos sugeridos

- Sustituir los scrapers sintÃ©ticos por implementaciones reales utilizando la
  misma interfaz.
- AÃ±adir monitorizaciÃ³n avanzada (metrics server, dashboard) si se requiere.
- Integrar tareas de respaldo automÃ¡tico para `orchestrator.db` y los CSV.
- Crear servicios `systemd` apoyÃ¡ndose en el CLI existente (`Controlador.py`
  actÃºa como alias para compatibilidad con la generaciÃ³n anterior).

Con estos elementos el repositorio queda listo para ejecutar flujos de scraping
robustos, reanudables y documentados, ademÃ¡s de servir como base para nuevas
integraciones empresariales.
