<?xml version="1.0" encoding="utf-8"?>
<Project>
  <Your_Goal>
    Analyze this folder and repair its script. It contains a Scraping orchestration project with Python scripts.
    Inside the "scrapers" folder, there are several scrapers that take URLs from the individual CSVs in the "urls" folder
    to scrape information from various web pages.
  </Your_Goal>
“https://www.inmuebles24.com/bodegas-comerciales-en-venta-en-zapopan.html” y en cada url hay varias paginas https://www.inmuebles24.com/bodegas-comerciales-en-venta-en-zapopan-pagina-2.html
Hay url que tienen una pagina o 30 paginas y algunos ninguna.
Carpeta de URLS y Carpeta de Variables., todos los url tienen la forma 

  <Project_Goal>
    It will run multiple processes in parallel daily, re-running every 15 days (or twice a month) to keep the data updated.
    The automatic chaining ensures a constant flow, creating a robust infrastructure for comprehensive daily data collection.
    I need to use only 80% of the hardware.
  </Project_Goal>

  <Hardware>
     Server: Dell T710.
     Software: Ubuntu 24.04 LTS
     CPU: Xeon E5620 (4 cores, 8 threads, Frequency 2.40 GHz, cache 12 MB Intel Smart Cache, Bus Speed 5.86 GT/s).
     RAM: 24 GB RAM(6x4gb).
     HDD: RAID0-10 HDDs.
     Router: Asus RT-AX82U · No Proxy.
     GPU: Matrox G200 integrado.
  </Hardware>

  <Variables>
    The composition of the CSVs in the "URLs" folder defines the hierarchy that the orchestration must follow.
    <Column1 name="PaginaWeb">Website: (Inm24, CyT, Lam, Mit, Prop, Tro)</Column1>
    <Column2 name="Ciudad">City: (Gdl, Zap, Tlaj, Tlaq, Ton, Salt, Zptl, IMem, Jnctl)</Column2>
    <Column3 name="Operacion">Operation: (Ven, Ren, Ven-d, Ven-r)</Column3>
    <Column4 name="ProductoPaginaWeb">ProductWebsite: (Dep, Cas, Ofc, Com, etc…)</Column4>
    <Column5 name="URL">Extraction URL (this column marks the order that the scrapes must follow according to the order of the URLs in the CSV; it is important to maintain this order) </Column5>
  </Variables>

  <Flow>
    The system should run several scrapers from the "scrapers" folder in parallel (4 or more at the same time with different “PaginaWeb”).
    For each Scraper, there is a CSV of the URLs it has to scrape for each “PaginaWeb&gt”.
    For example: the "cyt.py" Scraper has to scan the "cyt_urls.csv" file in the "URLs" folder (column 5 of the CSV),
    the "lam.py" Scraper must scan "lam_urls.csv", and so on, each Scraper with the CSV file named after it.
    Column 5 of each CSV has a desired order for the scrapes; this order must be respected and not altered.
  </Flow>

  <Flow_det>
    The "inm24_det.py" Scraper is different; it depends on the CSV generated by the "inm24.py" Scraper after its execution.
    Similarly, the "lam_det.py" Scraper should be guided by the CSV generated by the "lam.py" Scraper.
    Priority: inmuebles24.
  </Flow_det>

  <Folder_System>
    The backup of the CSVs with the information obtained by the scrapes must follow the following folder system.
    Example:
    <Example>
      If it is the first time implementing the "cyt.py" Scraper and we are in September 2025, and the Scraper scraped
      Apartments for Sale in Guadalajara (url: https://www.casasyterrenos.com/jalisco/guadalajara/departamentos/venta),
      it should save the generated CSV in the folder:
      <Path>data/CyT/Gdl/Ven/Dep/Sep25/01</Path>
      After 15 days, when the scrap is run again with the same variables, it will now be saved in:
      <Path>Base_de_Datos/CyT/Gdl/Ven/Dep/Sep25/01</Path>
      Folder structure: “PaginaWeb”/”Ciudad”/”Operacion”/”ProductoPaginaWeb”/”Mes;Año”/“1st Scrap or 2nd Scrap of the Month”;
      CSV name example:
      <FileName>CyT_Gdl_Ven_Dep_Sep25_01.csv</FileName>
      Format: “PaginaWeb”_”Ciudad”_”Operacion”_”ProductoPaginaWeb”_”Mes;Año”_”Scrap of the Month”.csv
    </Example>
  </Folder_System>

  <Folder_System_det>
    The "inm24.py" Scraper is different. Since it will only create an intermediate CSV, it will save its CSV in the same folders as the others,
    but its CSV will be named:
    <FileName>Inm24URL_Gdl_Ven_Dep_Sep25_01.csv</FileName>
    Example flow:
    <Step>
      After "inm24.py" finishes, "inm24_det.py" must be launched. The det scraper will look for the "inm24.py" CSV
      Inm24URL_Gdl_Ven_Dep_Sep25_01.csv in the folder data/Inm24/Gdl/Ven/Dep/Sep25/01,
      and the CSV generated by "inm24_det.py" would go to the same folder and be named:
      <FileName>Inm24_Gdl_Ven_Dep_Sep25_01.csv</FileName>
    </Step>
    The same dynamic applies to "lam.py" and "lam_det.py".
  </Folder_System_det>

  <Your_Goal_Orquestation>
    Review and repair everything: connections, readme files, orchestration files, automations, controllers, configurations,
    scrapers, scripts, utilities, nomenclature, etc.
  </Your_Goal_Orquestation>

  <Your_Max_Goal>
    I need you to review the scripts in the "scrapers" folder, their connections and synergies, and the CSV generations,
    ensuring everything is aligned with the folder system and its connections, variables, and nomenclature.
    Review their coordination with the project files and orchestration files, and their functionality.
  </Your_Max_Goal>

  <Monitor>
    I need to keep track of the progress of finished scraps, identify a "Scrap of the Month" from the completed jobs,
    and know which scraps are currently in process, which are queued to start, and which ones have not run at all.
    Additionally, I need any other utilities or tools that might be necessary for controlling and monitoring the orchestration (in the terminal).
    I would like to maintain the log for the scraps within the same CSV file from which the URLs are sourced.
  </Monitor>
</Project>
